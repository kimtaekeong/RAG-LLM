import os
import streamlit as st
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import ChatMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders.unstructured import UnstructuredFileLoader
from langchain_community.vectorstores.faiss import FAISS
from langserve import RemoteRunnable
from langchain_openai import ChatOpenAI
from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from datetime import datetime

# â­ï¸ Embedding ì„¤ì •
USE_BGE_EMBEDDING = True

if not USE_BGE_EMBEDDING:
    os.environ["OPENAI_API_KEY"] = "OPENAI API KEY ì…ë ¥"

LANGSERVE_ENDPOINT = "http://localhost/chat/"

# í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(".cache"):
    os.mkdir(".cache")
if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")
if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
RAG_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì…ë‹ˆë‹¤. ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”.
Question: {question}
Context: {context}
Answer:"""

st.set_page_config(page_title="OLLAMA Local ëª¨ë¸ í…ŒìŠ¤íŠ¸", page_icon="ğŸ’¬")
st.title("OLLAMA Local ëª¨ë¸ í…ŒìŠ¤íŠ¸")

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    ]

def print_history():
    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)

def add_history(role, content):
    st.session_state.messages.append(ChatMessage(role=role, content=content))

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

@st.cache_resource(show_spinner="Embedding files...")
def embed_files(files):
    retrievers = []
    for file in files:
        file_content = file.read()
        file_path = f"./.cache/files/{file.name}"
        with open(file_path, "wb") as f:
            f.write(file_content)

        cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n\n", "\n", "(?<=\. )", " ", ""],
            length_function=len,
        )
        loader = UnstructuredFileLoader(file_path)
        docs = loader.load_and_split(text_splitter=text_splitter)

        if USE_BGE_EMBEDDING:
            model_name = "BAAI/bge-m3"
            model_kwargs = {"device": "cuda"}
            encode_kwargs = {"normalize_embeddings": True}
            embeddings = HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs=model_kwargs,
                encode_kwargs=encode_kwargs,
            )
        else:
            embeddings = OpenAIEmbeddings()

        cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
        vectorstore = FAISS.from_documents(docs, embedding=cached_embeddings)
        retriever = vectorstore.as_retriever()
        retrievers.append(retriever)

    return retrievers

with st.sidebar:
    files = st.file_uploader(
        "íŒŒì¼ ì—…ë¡œë“œ",
        type=["pdf", "txt" ],
        accept_multiple_files=True
    )


print("test;",files)

if files:
    retrievers = embed_files(files)

print_history()

if user_input := st.chat_input():
    add_history("user", user_input)
    st.chat_message("user").write(user_input)
    with st.chat_message("assistant"):
        ollama = RemoteRunnable(LANGSERVE_ENDPOINT)
        chat_container = st.empty()

        if files:

            prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)

            # ëª¨ë“  retrieverë¥¼ ê²°í•©í•˜ì—¬ ë¬¸ë§¥ì„ ë§Œë“­ë‹ˆë‹¤.
            combined_docs = []
            for retriever in retrievers:
                retrieved_docs = retriever.get_relevant_documents(user_input)
                combined_docs.extend(retrieved_docs)

            context = format_docs(combined_docs)
            print("ë¬¸ì„œ:", context)

            rag_chain = (
                prompt | ollama | StrOutputParser()
            )
            answer = rag_chain.stream({"question": user_input, "context": context})
            chunks = []
            for chunk in answer:
                chunks.append(chunk)
                chat_container.markdown("".join(chunks))
            add_history("ai", "".join(chunks))

        

        else:
            prompt = ChatPromptTemplate.from_template(
                "ë‹¤ìŒì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”:\n{input}"
            )
            chain = prompt | ollama | StrOutputParser()
            answer = chain.stream(user_input)
            chunks = []
            for chunk in answer:
                chunks.append(chunk)
                chat_container.markdown("".join(chunks))
            add_history("ai", "".join(chunks))

        current_time = datetime.now().strftime("%Y:%m:%d:%H:%M")

        log_file_path = "dat_log.log"
        with open(log_file_path, 'a' if os.path.exists(log_file_path) else 'w') as log_file:
            log_file.write(f"{current_time} - '{user_input}' \n '{''.join(chunks)}'\n")

